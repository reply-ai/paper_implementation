{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T00:44:04.539577Z",
     "start_time": "2019-01-17T00:44:03.586922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages Imported\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import string\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from TextLoader import *\n",
    "from Hangulpy import *\n",
    "print (\"Packages Imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T00:44:04.572042Z",
     "start_time": "2019-01-17T00:44:04.540929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "type of 'data_loader' is <class 'dict'>, length is 75\n",
      "\n",
      "\n",
      "data_loader.vocab looks like \n",
      "{'ᴥ': 0, ' ': 1, 'ㅇ': 2, 'ㅏ': 3, 'ㄴ': 4, 'ㄹ': 5, 'ㅣ': 6, 'ㄱ': 7, 'ㅡ': 8, 'ㅗ': 9, 'ㅅ': 10, 'ㅓ': 11, 'ㅁ': 12, 'ㄷ': 13, 'ㅎ': 14, 'ㅈ': 15, 'ㅜ': 16, 'ㅂ': 17, 'ㅕ': 18, '\\n': 19, 'ㅔ': 20, 'ㅐ': 21, 'ㅆ': 22, 'ㅊ': 23, '.': 24, 'ㅢ': 25, 'ㅘ': 26, ',': 27, '\"': 28, 'ㄲ': 29, 'ㅌ': 30, 'ㅍ': 31, 'ㄸ': 32, 'ㅚ': 33, 'ㅑ': 34, 'ㅟ': 35, 'ㅝ': 36, 'ㅠ': 37, 'ㅛ': 38, 'ㅖ': 39, 'ㅉ': 40, '?': 41, 'ㅋ': 42, 'ㅄ': 43, 'ㄶ': 44, '(': 45, ')': 46, 'ㅃ': 47, 'ㄺ': 48, \"'\": 49, 'ㄵ': 50, 'ㅀ': 51, '!': 52, 'ㅙ': 53, 'ㄼ': 54, 'ㄻ': 55, 'ㄿ': 56, ':': 57, '1': 58, '6': 59, '2': 60, 'ㄳ': 61, 'ㅞ': 62, '9': 63, '5': 64, '-': 65, '4': 66, '3': 67, '8': 68, '7': 69, '_': 70, 'ㄾ': 71, 'ㅒ': 72, '0': 73, '>': 74} \n",
      "\n",
      "\n",
      "type of 'data_loader.chars' is <class 'tuple'>, length is 75\n",
      "\n",
      "\n",
      "data_loader.chars looks like \n",
      "('ᴥ', ' ', 'ㅇ', 'ㅏ', 'ㄴ', 'ㄹ', 'ㅣ', 'ㄱ', 'ㅡ', 'ㅗ', 'ㅅ', 'ㅓ', 'ㅁ', 'ㄷ', 'ㅎ', 'ㅈ', 'ㅜ', 'ㅂ', 'ㅕ', '\\n', 'ㅔ', 'ㅐ', 'ㅆ', 'ㅊ', '.', 'ㅢ', 'ㅘ', ',', '\"', 'ㄲ', 'ㅌ', 'ㅍ', 'ㄸ', 'ㅚ', 'ㅑ', 'ㅟ', 'ㅝ', 'ㅠ', 'ㅛ', 'ㅖ', 'ㅉ', '?', 'ㅋ', 'ㅄ', 'ㄶ', '(', ')', 'ㅃ', 'ㄺ', \"'\", 'ㄵ', 'ㅀ', '!', 'ㅙ', 'ㄼ', 'ㄻ', 'ㄿ', ':', '1', '6', '2', 'ㄳ', 'ㅞ', '9', '5', '-', '4', '3', '8', '7', '_', 'ㄾ', 'ㅒ', '0', '>') \n"
     ]
    }
   ],
   "source": [
    "## Load dataset using TextLoader\n",
    "data_dir    = \"data/\"\n",
    "batch_size  = 50\n",
    "seq_length  = 50\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# This makes \"vocab.pkl\" and \"data.npy\" in \"data/nine_dreams\"   \n",
    "#  from \"data/nine_dreams/input.txt\" \n",
    "vocab_size = data_loader.vocab_size\n",
    "vocab = data_loader.vocab\n",
    "chars = data_loader.chars\n",
    "print ( \"type of 'data_loader' is %s, length is %d\" \n",
    "       % (type(data_loader.vocab), len(data_loader.vocab)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.vocab looks like \\n%s \" %\n",
    "       (data_loader.vocab))\n",
    "print ( \"\\n\" )\n",
    "print ( \"type of 'data_loader.chars' is %s, length is %d\" \n",
    "       % (type(data_loader.chars), len(data_loader.chars)) )\n",
    "print ( \"\\n\" )\n",
    "print (\"data_loader.chars looks like \\n%s \" % (data_loader.chars,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T00:44:08.214556Z",
     "start_time": "2019-01-17T00:44:04.576211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-0718377a8a3b>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "Network Ready\n"
     ]
    }
   ],
   "source": [
    "## Define network\n",
    "rnn_size   = 512\n",
    "num_layers = 3\n",
    "grad_clip  = 5.\n",
    "\n",
    "_batch_size = 1\n",
    "_seq_length = 1\n",
    "\n",
    "vocab_size = data_loader.vocab_size\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    # Select RNN Cell\n",
    "    unitcell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([unitcell] * num_layers)\n",
    "    # Set paths to the graph \n",
    "    input_data = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    targets    = tf.placeholder(tf.int32, [_batch_size, _seq_length])\n",
    "    initial_state = cell.zero_state(_batch_size, tf.float32)\n",
    "\n",
    "    # Set Network\n",
    "    with tf.variable_scope('rnnlm'):\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size])\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "            inputs = tf.split(tf.nn.embedding_lookup(embedding, input_data), _seq_length, 1)\n",
    "            inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "            \n",
    "    # Loop function for seq2seq\n",
    "    def loop(prev, _):\n",
    "        prev = tf.nn.xw_plus_b(prev, softmax_w, softmax_b)\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "    # Output of RNN \n",
    "    outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state\n",
    "                                , cell, loop_function=None, scope='rnnlm')\n",
    "#     output = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])\n",
    "    output = tf.reshape(outputs, [-1, rnn_size])\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    # Next word probability \n",
    "    probs = tf.nn.softmax(logits)\n",
    "    # Define LOSS\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], # Input\n",
    "        [tf.reshape(targets, [-1])], # Target\n",
    "        [tf.ones([_batch_size * _seq_length])], # Weight \n",
    "        vocab_size)\n",
    "    # Define Optimizer\n",
    "    cost = tf.reduce_sum(loss) / _batch_size / _seq_length\n",
    "    final_state = last_state\n",
    "    lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    _optm = tf.train.AdamOptimizer(lr)\n",
    "    optm = _optm.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "print (\"Network Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T00:44:08.223693Z",
     "start_time": "2019-01-17T00:44:08.216272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling function done.\n"
     ]
    }
   ],
   "source": [
    "## Sample!!\n",
    "\n",
    "def sample( sess, chars, vocab, __probs, num=200, prime=u'ㅇㅗᴥㄴㅡㄹᴥ '):\n",
    "    state = sess.run(cell.zero_state(1, tf.float32))\n",
    "    _probs = __probs\n",
    "    prime = list(prime)\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [state] = sess.run([final_state], feed)\n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        weights = weights / np.sum(weights) \n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for n in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {input_data: x, initial_state:state}\n",
    "        [_probsval, state] = sess.run([_probs, final_state], feed)\n",
    "        p = _probsval[0]\n",
    "        sample = int(np.random.choice(len(p), p=p))\n",
    "        # sample = weighted_pick(p)\n",
    "        # sample = np.argmax(p)\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret\n",
    "print (\"sampling function done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T00:44:14.524343Z",
     "start_time": "2019-01-17T00:44:08.225423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prime Text : 누구  => ㄴㅜᴥㄱㅜᴥ \n",
      "WARNING:tensorflow:From /home/jwserver/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-cd6cbf0c78cf>:9: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "data/nine_dreams/model.ckpt-99000\n",
      "INFO:tensorflow:Restoring parameters from data/nine_dreams/model.ckpt-99000\n",
      "SAMPLED TEXT = ['ㄴ', 'ㅜ', 'ᴥ', 'ㄱ', 'ㅜ', 'ᴥ', ' ', 'ᴥ', 'ㅡ', 'ㅎ', 'ㅣ', 'ㅏ', 'ㄴ', ' ', 'ㄱ', 'ᴥ', 'ᴥ', 'ㅓ', 'ㅇ', 'ㄱ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㄱ', 'ㄴ', 'ᴥ', 'ᴥ', ' ', ' ', ' ', ' ', 'ㅇ', 'ㅣ', ' ', 'ㄴ', ' ', 'ᴥ', 'ㅓ', 'ㅅ', 'ᴥ', 'ㅓ', 'ㅢ', 'ㅓ', 'ㅇ', 'ㅇ', 'ㅈ', 'ᴥ', 'ㅇ', 'ㅇ', 'ᴥ', 'ㅎ', 'ᴥ', 'ᴥ', 'ㄸ', 'ㅅ', ' ', 'ㅡ', 'ㄹ', 'ㅇ', 'ㄴ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㄱ', 'ㅅ', ' ', 'ㅣ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅇ', 'ᴥ', 'ㅇ', 'ㅗ', 'ㄹ', ' ', 'ᴥ', 'ㅇ', 'ㅣ', 'ᴥ', 'ᴥ', 'ㅊ', 'ㄱ', 'ᴥ', 'ㄷ', 'ㄱ', 'ㅘ', 'ᴥ', 'ㅔ', 'ㄴ', 'ㄴ', 'ㅇ', 'ᴥ', 'ㄷ', 'ㅇ', 'ᴥ', 'ㄱ', 'ㅅ', 'ㅅ', 'ㅡ', 'ㅇ', 'ㅓ', 'ㅜ', 'ㅣ', 'ㄹ', 'ㅗ', 'ᴥ', 'ㅔ', 'ㅘ', 'ㅗ', 'ㄱ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄱ', 'ㄱ', 'ㅛ', 'ㅇ', 'ㅡ', 'ㅅ', 'ᴥ', 'ㅘ', 'ㅆ', ' ', 'ㅇ', 'ㅐ', 'ㅡ', 'ᴥ', 'ㄴ', 'ᴥ', 'ㅁ', 'ㅇ', 'ᴥ', 'ᴥ', ' ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㅓ', 'ㅔ', 'ᴥ', ' ', 'ㄱ', 'ᴥ', 'ᴥ', 'ㅜ', 'ㅗ', 'ㅅ', 'ㅇ', 'ㅅ', 'ㅅ', ')', 'ᴥ', 'ㄱ', 'ᴥ', 'ᴥ', 'ㅣ', 'ᴥ', 'ㅏ', 'ᴥ', 'ㄱ', 'ㅅ', 'ᴥ', 'ᴥ', 'ㅕ', 'ᴥ', 'ᴥ', ' ', 'ㅚ', 'ㅇ', 'ᴥ', 'ᴥ', ' ', ' ', 'ㅏ', 'ㅇ', 'ㄴ', 'ᴥ', 'ㄱ', 'ㄴ', 'ㅇ', ' ', 'ㅗ', 'ᴥ', 'ㅁ', '\\n', 'ᴥ', 'ㅅ', 'ㅇ', 'ㅁ', 'ㅓ', 'ㅎ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅁ', 'ㅜ', 'ㅁ', 'ㅣ', 'ᴥ', ')', 'ᴥ', 'ㅔ', 'ᴥ', 'ㅜ', 'ᴥ', 'ᴥ', ' ', ' ', 'ᴥ', 'ㄹ', ' ', 'ㅡ', 'ㅏ', 'ㄴ', 'ᴥ', 'ᴥ', 'ㅖ', 'ᴥ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㄷ', 'ㅎ', 'ㅓ', 'ᴥ', 'ᴥ', 'ㄷ', 'ㅏ', 'ㅇ', 'ㅡ', 'ㅇ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅄ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅐ', 'ㅡ', 'ㄱ', 'ㄹ', 'ㅎ', 'ᴥ', 'ㄴ', 'ㅏ', 'ㅡ', 'ㅠ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㄷ', 'ᴥ', 'ᴥ', 'ㄹ', 'ㅇ', 'ㄴ', '\\n', 'ᴥ', ' ', 'ㄴ', 'ᴥ', 'ㅇ', 'ㄹ', 'ㄹ', 'ㅂ', 'ᴥ', 'ㅇ', 'ㄱ', ' ', 'ㄱ', 'ㄹ', 'ㅏ', ' ', 'ㅇ', 'ᴥ', 'ᴥ', 'ᴥ', ' ', 'ㅡ', 'ㅡ', 'ᴥ', 'ㅣ', 'ㅘ', 'ㅣ', 'ㄹ', 'ᴥ', ' ', 'ㅌ', 'ㅏ', 'ㅏ', 'ㅇ', 'ᴥ', 'ㅏ', 'ᴥ', 'ㄱ', 'ᴥ', 'ᴥ', 'ㄴ', 'ㄷ', 'ㅜ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅏ', 'ㅡ', 'ㅡ', 'ㅜ', 'ㅡ', 'ᴥ', 'ᴥ', 'ㄷ', 'ᴥ', 'ㅣ', 'ᴥ', 'ᴥ', 'ㅅ', ' ', 'ㅕ', ' ', 'ㄷ', 'ᴥ', 'ㅈ', 'ㅇ', 'ᴥ', ' ', 'ㅇ', 'ᴥ', 'ㅡ', ' ', 'ㅇ', 'ᴥ', 'ㅇ', 'ㅏ', 'ᴥ', 'ㅣ', 'ㄴ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅢ', '\\n', 'ㅁ', 'ㅂ', 'ㄹ', 'ㅡ', 'ᴥ', 'ㅗ', 'ᴥ', ' ', 'ᴥ', 'ㅣ', 'ㅏ', 'ᴥ', 'ᴥ', 'ᴥ', '.', 'ᴥ', 'ㅎ', 'ᴥ', ' ', 'ㅏ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅔ', 'ㄴ', 'ㅂ', 'ㅂ', 'ㅣ', 'ㅆ', 'ㅓ', 'ㅏ', ' ', 'ᴥ', 'ㄹ', 'ᴥ', 'ᴥ', ' ', 'ㄴ', 'ᴥ', 'ㄹ', ' ', 'ㅁ', 'ᴥ', 'ㅕ', 'ᴥ', ' ', ' ', 'ᴥ', 'ㅠ', 'ㄴ', 'ㄷ', 'ㅜ', 'ㄹ', 'ㅗ', 'ㅏ', 'ㄷ', 'ㅇ', 'ᴥ', ' ', 'ㅣ', 'ㅅ', 'ㅜ', 'ㅂ', 'ㄹ', 'ㄴ', ' ', 'ㅇ', 'ㅊ', 'ᴥ', 'ㅈ', 'ㅏ', 'ㅇ', 'ㅂ', 'ᴥ', 'ᴥ', 'ㅅ', 'ㅘ', 'ㅔ', 'ㅁ', 'ㅓ', ' ', 'ᴥ', 'ㄴ', 'ㅇ', 'ᴥ', 'ㄴ', 'ㄴ', 'ㅎ', 'ㅏ', 'ᴥ', 'ㄴ', 'ᴥ', 'ㄷ', 'ㄹ', 'ㅇ', 'ᴥ', 'ㅁ', 'ㅣ', 'ᴥ', 'ᴥ', ' ', 'ㅇ', 'ㅓ', 'ㅅ', ' ', 'ㄴ', 'ㅜ', 'ᴥ', 'ㅣ', 'ㄷ', 'ㅇ', 'ㅅ', '\\n', 'ㅇ', 'ㅓ', 'ㅅ', 'ᴥ', 'ㄹ', 'ᴥ', 'ㅏ', 'ㅍ', 'ᴥ', 'ㄴ', 'ㅔ', 'ᴥ', 'ㅆ', 'ㅓ', 'ㅡ', 'ㄱ', 'ㅐ', 'ㅣ', ' ', 'ㄸ', 'ㅎ', 'ㅓ', 'ㅡ', 'ᴥ', 'ᴥ', 'ㄷ', 'ㅡ', 'ᴥ', ' ', 'ᴥ', ' ', 'ᴥ', ' ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅎ', 'ㅣ', 'ᴥ', ' ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅣ', 'ㅏ', 'ㅐ', 'ㅢ', 'ㅡ', 'ㅣ', '.', 'ㅈ', 'ᴥ', 'ㅣ', ' ', 'ᴥ', 'ㄹ', 'ㅣ', 'ᴥ', 'ㅐ', 'ㄱ', 'ᴥ', 'ㅜ', 'ᴥ', 'ㅉ', 'ㄹ', 'ᴥ', 'ㅏ', ' ', 'ㅅ', 'ㅓ', 'ㅇ', 'ᴥ', 'ㅎ', 'ㅁ', 'ᴥ', 'ㄴ', 'ㅇ', 'ㄴ', 'ㅂ', 'ㄴ', ' ', 'ㅁ', 'ㅇ', 'ㅣ', 'ㅆ', 'ㅗ', 'ᴥ', 'ᴥ', ' ', ' ', 'ㄹ', 'ㅎ', 'ㅁ', 'ㅔ', 'ㅇ', 'ㄱ', 'ㅇ', 'ㄴ', ' ', 'ᴥ', 'ㅇ', ' ', 'ㅇ', 'ㅏ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄴ', 'ㅏ', 'ㅏ', 'ㅇ', 'ㅇ', 'ㅜ', 'ᴥ', 'ㅇ', 'ㅔ', 'ᴥ', 'ᴥ', 'ㅈ', 'ㅎ', 'ㅁ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄴ', 'ㄷ', ' ', 'ᴥ', 'ㅇ', 'ㅎ', 'ㅂ', 'ㄱ', 'ㄴ', 'ㄷ', 'ㄱ', 'ㅢ', 'ㅕ', 'ㅐ', 'ᴥ', 'ㅣ', 'ㅏ', 'ᴥ', 'ㄴ', 'ᴥ', 'ᴥ', 'ㄴ', 'ㅇ', ' ', 'ㅜ', ' ', 'ㅅ', '\\n', 'ㄴ', ' ', 'ㅗ', 'ㅇ', 'ㄴ', 'ᴥ', 'ㅅ', 'ㅇ', 'ᴥ', 'ㅅ', 'ㅈ', 'ㄹ', ',', 'ㅅ', ' ', 'ㅣ', 'ㅣ', ' ', 'ᴥ', 'ᴥ', 'ㅏ', 'ㄱ', 'ᴥ', 'ᴥ', ' ', 'ㅣ', 'ᴥ', 'ᴥ', ' ', 'ㅣ', ' ', 'ㅎ', 'ㅓ', 'ᴥ', 'ㄴ', 'ㅓ', 'ㅓ', 'ᴥ', 'ㄴ', 'ㅇ', 'ᴥ', 'ㅏ', 'ㄹ', 'ᴥ', 'ㅅ', 'ᴥ', ' ', ' ', 'ㅂ', 'ᴥ', 'ㅣ', 'ㅣ', 'ㅔ', 'ㅆ', 'ᴥ', 'ᴥ', ' ', 'ㄱ', 'ㅏ', 'ᴥ', 'ᴥ', ' ', 'ㅅ', ' ', 'ᴥ', 'ᴥ', 'ᴥ', ' ', 'ㅣ', 'ᴥ', 'ᴥ', 'ㅗ', 'ㅗ', 'ㅇ', 'ㅓ', 'ᴥ', 'ㅈ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅁ', 'ㅅ', 'ㄹ', 'ㅇ', ' ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅂ', 'ㄴ', 'ᴥ', 'ᴥ', ' ', 'ㅅ', ' ', 'ㅣ', 'ㅘ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㄴ', 'ㅣ', 'ㅎ', 'ㅜ', 'ᴥ', 'ㄴ', 'ᴥ', 'ㅈ', 'ㄱ', 'ㅈ', 'ㅇ', 'ㅇ', 'ㄴ', 'ㅏ', 'ㅣ', ' ', ' ', 'ㅇ', 'ㅅ', 'ㅑ', ' ', 'ㅇ', ' ', 'ㄷ', 'ㅇ', 'ㄹ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', ' ', ' ', 'ㄱ', 'ᴥ', 'ㅇ', 'ㄱ', 'ㄱ', 'ㅅ', 'ㅇ', 'ㄹ', 'ㅜ', 'ㅐ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄹ', 'ㅣ', ' ', 'ㅓ', 'ㅎ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅣ', 'ㄴ', 'ㅏ', 'ᴥ', 'ㄴ', 'ㅎ', 'ᴥ', 'ㄹ', 'ㄱ', 'ㅗ', 'ㅏ', 'ㅗ', ' ', 'ㅏ', 'ㄴ', 'ㄴ', 'ㅣ', 'ㄱ', 'ㄱ', 'ᴥ', 'ㅅ', 'ᴥ', 'ㅂ', 'ㅇ', 'ᴥ', 'ᴥ', ' ', ' ', 'ㅏ', 'ᴥ', 'ㄱ', 'ㅏ', 'ㅕ', 'ㅅ', 'ᴥ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㅏ', 'ㅠ', 'ㅗ', 'ㄴ', 'ᴥ', 'ㅏ', 'ㅅ', 'ㅁ', 'ㅇ', 'ㅎ', 'ㄴ', 'ㅏ', 'ㅊ', 'ㅗ', 'ᴥ', 'ᴥ', '\"', 'ᴥ', 'ㅁ', 'ㅡ', 'ㄱ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㄴ', 'ᴥ', 'ㅓ', 'ㅕ', 'ㅇ', 'ᴥ', 'ㄷ', 'ㅇ', 'ㄴ', ' ', 'ㅗ', 'ㅇ', 'ᴥ', 'ㅡ', 'ㅌ', 'ᴥ', 'ㅓ', 'ㅁ', ' ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅆ', 'ᴥ', 'ㄴ', 'ㅇ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅈ', 'ㄹ', 'ㅍ', 'ㅇ', 'ㅠ', 'ㅓ', 'ᴥ', 'ㄹ', 'ㅏ', 'ᴥ', 'ㅔ', 'ㅔ', 'ᴥ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㅣ', 'ᴥ', 'ㅂ', 'ㅏ', 'ᴥ', 'ㅆ', 'ㅣ', 'ㅁ', ' ', 'ㅜ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㅣ', 'ᴥ', 'ᴥ', ' ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅂ', 'ㅓ', 'ᴥ', 'ᴥ', 'ㅢ', 'ㅐ', 'ㅂ', 'ᴥ', 'ᴥ', 'ㅗ', 'ㅜ', 'ㅇ', 'ㅣ', 'ㄹ', 'ㅗ', 'ᴥ', 'ㄹ', 'ㅕ', ' ', 'ㅔ', 'ㅐ', 'ᴥ', 'ㄴ', 'ㅓ', 'ㅊ', 'ㅇ', ' ', 'ㅇ', 'ㅓ', ' ', 'ᴥ', 'ㄹ', 'ㅇ', 'ㄴ', ',', 'ᴥ', 'ㅣ', ' ', 'ㅈ', ' ', 'ㅇ', 'ㅅ', 'ᴥ', 'ㄴ', 'ᴥ', 'ㄹ', 'ᴥ', 'ㄶ', ' ', 'ㅇ', 'ᴥ', 'ㅐ', 'ㅁ', 'ᴥ', 'ㄹ', 'ᴥ', 'ᴥ', 'ㅔ', 'ㅏ', 'ᴥ', 'ㄴ', 'ㄷ', 'ㅜ', 'ᴥ', ' ', 'ㄴ', 'ㅇ', 'ㅁ', 'ㅇ', 'ㅜ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅇ', 'ㅕ', ' ', 'ᴥ', 'ᴥ', ' ', 'ㅢ', 'ㅓ', 'ᴥ', 'ㅗ', 'ㅏ', ' ', 'ㄹ', ' ', 'ㄴ', 'ᴥ', 'ᴥ', 'ㅏ', ' ', 'ᴥ', 'ㅈ', 'ᴥ', 'ㄱ', 'ㅏ', ' ', 'ㅣ', 'ㅐ', 'ㄴ', 'ㅁ', 'ㄱ', 'ᴥ', 'ㅅ', ' ', 'ㅡ', 'ᴥ', 'ㅏ', 'ㄹ', 'ᴥ', '\\n', 'ㅓ', 'ㅜ', 'ㅓ', 'ㄱ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅓ', 'ㅢ', 'ㄱ', 'ㄿ', 'ㅏ', 'ㅣ', 'ㅇ', 'ㅆ', 'ᴥ', 'ㅁ', '\\n', 'ᴥ', 'ᴥ', 'ㅇ', 'ᴥ', ' ', 'ᴥ', 'ㅇ', 'ㄴ', ' ', 'ㅛ', 'ㄱ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅇ', 'ㅇ', ' ', 'ᴥ', 'ㄹ', 'ㅓ', 'ㄴ', ' ', ' ', 'ᴥ', 'ᴥ', 'ㅗ', ' ', 'ㅅ', 'ᴥ', 'ㅊ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅏ', 'ㅏ', 'ᴥ', 'ㅡ', 'ㄷ', 'ㅕ', 'ㅜ', 'ᴥ', 'ㅈ', 'ᴥ', 'ㅈ', 'ㅇ', ' ', 'ㅗ', 'ㅏ', 'ㅇ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅇ', 'ㅣ', 'ㅔ', 'ㅇ', 'ㄱ', 'ᴥ', 'ㅏ', 'ᴥ', 'ㅜ', 'ㅡ', 'ㅔ', 'ᴥ', 'ㅎ', 'ㅇ', 'ᴥ', '\\n', 'ᴥ', ' ', 'ㅓ', 'ㅏ', 'ᴥ', 'ㄴ', 'ㄴ', ' ', 'ᴥ', 'ᴥ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㅏ', 'ㄹ', ' ', 'ㅗ', 'ㄹ', 'ㄱ', 'ㅈ', 'ᴥ', 'ㅎ', 'ᴥ', 'ㄴ', 'ㅔ', 'ㅡ', 'ㅗ', 'ㄹ', 'ᴥ', 'ㅡ', 'ㄱ', 'ㅐ', 'ㅗ', 'ㅡ', 'ㅗ', 'ㄴ', ' ', 'ᴥ', ' ', 'ㅇ', 'ㅏ', 'ᴥ', 'ㅁ', 'ㅓ', 'ㅢ', 'ㅣ', 'ㅣ', 'ᴥ', 'ㅂ', 'ㅔ', 'ㅏ', 'ᴥ', 'ㅡ', 'ㄹ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㄹ', 'ㅣ', 'ㄷ', 'ᴥ', 'ᴥ', 'ᴥ', ' ', 'ㅏ', 'ㅏ', 'ㅇ', 'ㅇ', 'ᴥ', 'ㄷ', 'ㅎ', 'ㅆ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅂ', 'ㄹ', 'ㅣ', 'ㅏ', 'ㅏ', 'ᴥ', ' ', 'ᴥ', ' ', 'ᴥ', 'ㅣ', 'ㅇ', 'ㅅ', 'ㅏ', ' ', 'ᴥ', 'ᴥ', ' ', 'ㅗ', 'ㅎ', 'ㅇ', ' ', 'ㅁ', 'ㅡ', ' ', 'ㅎ', 'ᴥ', 'ᴥ', ' ', 'ᴥ', 'ㄴ', 'ㄴ', 'ㅐ', ' ', 'ᴥ', 'ㅕ', 'ㅓ', 'ᴥ', 'ᴥ', 'ㅝ', 'ᴥ', 'ㅣ', 'ㄴ', 'ㅗ', 'ㅇ', 'ㅏ', 'ᴥ', 'ㅡ', 'ㅇ', 'ᴥ', 'ㅇ', 'ㄹ', 'ᴥ', 'ㄹ', 'ㄴ', 'ㅇ', 'ㅣ', 'ᴥ', 'ㅖ', 'ᴥ', 'ㅜ', 'ㄹ', 'ㄹ', 'ㄴ', 'ㅇ', 'ᴥ', 'ㅁ', ' ', 'ㄹ', 'ㅌ', 'ᴥ', 'ㄴ', 'ㄱ', 'ㅐ', 'ㅏ', 'ㄴ', 'ㄴ', 'ㄹ', 'ㅅ', 'ㅑ', 'ㅆ', 'ᴥ', 'ㄴ', 'ᴥ', 'ㅇ', 'ᴥ', 'ㄹ', 'ㅣ', 'ㅇ', 'ㅡ', 'ㅡ', 'ㅇ', 'ㅏ', 'ᴥ', 'ㅊ', 'ㄴ', 'ㅘ', 'ㅇ', 'ㄴ', 'ㄴ', 'ᴥ', 'ㄹ', 'ㅌ', 'ㅣ', 'ㅡ', 'ㅡ', 'ᴥ', 'ᴥ', 'ㄱ', 'ㅕ', 'ᴥ', 'ㅇ', 'ㅈ', 'ㅡ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㅅ', 'ㅇ', 'ㄴ', 'ㅇ', 'ㅑ', 'ㄱ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅔ', 'ᴥ', 'ㅕ', ' ', 'ᴥ', 'ㅜ', 'ㅁ', 'ᴥ', ' ', 'ㅁ', ' ', ' ', 'ㅓ', 'ㅡ', 'ㄴ', 'ㅎ', 'ᴥ', ' ', 'ㅁ', ' ', 'ᴥ', 'ㅂ', 'ㅓ', 'ㅡ', 'ㅁ', 'ㅡ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㅏ', 'ㅓ', 'ㅕ', 'ㅅ', 'ᴥ', ' ', 'ᴥ', 'ㅕ', 'ㅁ', ' ', 'ㅡ', 'ㅡ', 'ㄴ', 'ㄹ', 'ㅈ', 'ㅏ', 'ᴥ', ' ', 'ᴥ', 'ㄱ', 'ㅇ', 'ㅎ', 'ㅁ', 'ㅇ', 'ㅟ', 'ㅇ', ' ', 'ㅁ', 'ㅗ', ' ', 'ᴥ', 'ㅁ', ' ', ' ', ' ', 'ㅏ', 'ㅓ', 'ᴥ', 'ㅡ', 'ㄲ', ' ', 'ᴥ', 'ㅡ', 'ㅏ', 'ㅏ', 'ㄹ', 'ᴥ', 'ㄹ', ' ', ' ', 'ᴥ', 'ㅎ', '!', 'ㅜ', 'ᴥ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㅁ', 'ㅇ', 'ㅡ', 'ᴥ', 'ㅣ', 'ㅇ', 'ㅁ', 'ᴥ', 'ᴥ', 'ㅓ', 'ㄷ', 'ᴥ', 'ㅗ', 'ㅈ', 'ㅁ', 'ᴥ', 'ㄷ', 'ㅏ', 'ㄴ', 'ㄴ', 'ᴥ', 'ㅂ', 'ᴥ', 'ㅣ', 'ᴥ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅡ', 'ㅈ', 'ᴥ', 'ᴥ', 'ㅓ', 'ㅁ', ',', 'ᴥ', 'ㄱ', ' ', ',', ' ', '\\n', 'ᴥ', 'ㅏ', 'ㄹ', 'ㅇ', 'ᴥ', 'ㅕ', 'ᴥ', 'ㅣ', 'ㅅ', 'ㄱ', ' ', 'ㅎ', 'ㅅ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅜ', 'ᴥ', 'ㅓ', 'ㄴ', 'ᴥ', ' ', 'ㅇ', 'ㅡ', 'ㅈ', ' ', 'ᴥ', 'ㅇ', 'ㅅ', 'ㅇ', 'ㄱ', 'ᴥ', ' ', 'ㅋ', 'ᴥ', 'ㄴ', 'ㄹ', 'ㅟ', 'ㅣ', 'ㅔ', 'ㄴ', 'ᴥ', 'ᴥ', 'ㅏ', '\\n', 'ㅂ', ' ', 'ㅘ', 'ᴥ', 'ㅛ', '\\n', 'ㅕ', 'ㄱ', 'ᴥ', 'ㅡ', 'ᴥ', 'ᴥ', 'ㅐ', 'ㅂ', ' ', ' ', 'ᴥ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅏ', 'ㅈ', 'ㅆ', 'ㅓ', 'ㅏ', 'ㅆ', 'ᴥ', 'ㄴ', 'ㅏ', 'ㅅ', 'ㅇ', '.', 'ㄴ', 'ᴥ', 'ㄴ', 'ㅈ', 'ᴥ', 'ㅣ', 'ㄴ', 'ㅅ', 'ㅈ', 'ㄹ', 'ㅗ', 'ᴥ', 'ㅓ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅣ', 'ㅣ', 'ㅁ', 'ㅗ', 'ㄴ', 'ㅇ', 'ㅛ', 'ㅇ', 'ㅈ', 'ㄷ', 'ᴥ', 'ᴥ', 'ㅣ', 'ᴥ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅓ', ' ', 'ㅎ', 'ᴥ', 'ㅇ', 'ㅡ', 'ㅏ', 'ㅇ', 'ᴥ', 'ㅇ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㄷ', 'ㅇ', 'ㅇ', 'ᴥ', 'ㅊ', 'ᴥ', 'ㅏ', 'ㅣ', ' ', 'ᴥ', 'ㅈ', 'ᴥ', 'ㄴ', ' ', 'ㅕ', 'ㅅ', 'ㅇ', 'ㅅ', 'ㅣ', 'ᴥ', 'ᴥ', 'ㄹ', 'ㄹ', 'ㅏ', 'ㅎ', 'ᴥ', 'ㅕ', 'ㅣ', 'ㄴ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄹ', 'ㅓ', 'ㄲ', 'ㅡ', 'ㅏ', 'ㅇ', 'ᴥ', ' ', ' ', ' ', 'ㅕ', '\\n', 'ㅂ', 'ᴥ', 'ㄱ', 'ᴥ', 'ㄱ', 'ㅣ', 'ㅣ', 'ᴥ', 'ㅓ', 'ㄱ', 'ᴥ', 'ㅣ', ' ', 'ㅣ', ' ', 'ㅡ', 'ㅇ', 'ㅏ', 'ㄷ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄱ', 'ㅅ', 'ㅇ', 'ㅣ', 'ㅐ', 'ㄹ', 'ㅆ', 'ㅡ', 'ㄱ', 'ㅊ', 'ㅡ', 'ㅇ', '\\n', 'ㅇ', 'ㄴ', 'ᴥ', 'ᴥ', 'ㄱ', 'ㄴ', 'ㄹ', 'ㄴ', 'ㅇ', 'ᴥ', 'ㄹ', 'ㄹ', 'ㅠ', ' ', 'ㅇ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㅔ', 'ᴥ', 'ㄹ', 'ㅇ', 'ㅊ', ' ', 'ㅁ', 'ㅓ', 'ㄴ', 'ㅓ', 'ᴥ', 'ㅏ', 'ᴥ', 'ㅡ', 'ᴥ', 'ㅏ', 'ㅎ', 'ㄹ', 'ㄷ', ' ', ' ', 'ㅏ', 'ㅟ', 'ᴥ', 'ㅓ', 'ㅐ', 'ᴥ', 'ㅢ', 'ㅇ', 'ㅏ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅕ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅣ', ' ', 'ㅓ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅉ', 'ㅎ', ' ', 'ㅇ', 'ㄲ', 'ㅡ', 'ᴥ', 'ᴥ', 'ㅗ', ' ', 'ㅇ', 'ㅅ', 'ㄹ', 'ᴥ', 'ㅇ', 'ㄷ', 'ᴥ', 'ㅇ', 'ᴥ', 'ㅇ', 'ㅍ', ' ', 'ㅇ', 'ㅇ', 'ㄴ', 'ᴥ', 'ㅎ', 'ᴥ', 'ᴥ', '\\n', 'ᴥ', 'ㄱ', 'ᴥ', 'ㅁ', 'ㄴ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅏ', ' ', 'ㅏ', 'ㄴ', 'ㅇ', 'ㄹ', ' ', 'ㅡ', 'ᴥ', 'ㄴ', 'ㅅ', 'ㅗ', 'ㅇ', 'ㅏ', ' ', 'ㅇ', 'ᴥ', 'ᴥ', 'ᴥ', 'ᴥ', ' ', 'ㅗ', '\"', 'ㅔ', 'ㅓ', 'ㄹ', 'ᴥ', 'ㅏ', 'ㅅ', 'ᴥ', 'ㄹ', 'ㄹ', 'ㅜ', 'ㅂ', ' ', ' ', ' ', 'ㅅ', '\\n', '\"', 'ㅗ', 'ㄴ', 'ㅌ', 'ᴥ', 'ᴥ', 'ㅏ', 'ᴥ', 'ᴥ', 'ㄹ', 'ㅌ', 'ㅇ', 'ㄱ', 'ㅆ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㅇ', 'ㅔ', ',', 'ᴥ', ' ', 'ㅕ', 'ㅎ', 'ㅏ', 'ㅂ', 'ㄱ', ' ', 'ㄴ', 'ㅇ', 'ᴥ', 'ㄴ', 'ㅗ', ' ', 'ㅜ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄹ', 'ᴥ', 'ㄱ', 'ㄴ', 'ᴥ', 'ᴥ', 'ㅕ', 'ㄴ', 'ㅇ', 'ㄹ', 'ㅅ', 'ㅣ', 'ㄹ', 'ᴥ', 'ㅇ', ' ', 'ㄱ', 'ᴥ', 'ㅇ', ' ', 'ㅇ', 'ᴥ', ' ', 'ㅣ', 'ㅏ', 'ᴥ', 'ㅜ', 'ㅣ', 'ㄷ', ' ', 'ㅇ', 'ㄸ', 'ᴥ', 'ㅜ', 'ㅈ', 'ᴥ', 'ㅎ', '\\n', 'ᴥ', 'ㄷ', '\\n', 'ᴥ', 'ㄹ', 'ㅜ', 'ㅆ', 'ㄱ', 'ᴥ', 'ㄴ', 'ㅕ', 'ㄷ', 'ᴥ', 'ㅓ', 'ㄴ', 'ㅏ', 'ㅈ', ' ', 'ㅁ', 'ㅇ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄹ', 'ㄹ', 'ㅣ', 'ᴥ', 'ㄴ', 'ᴥ', 'ㅅ', 'ᴥ', 'ㅘ', 'ㅇ', '\\n', 'ㅏ', 'ᴥ', 'ᴥ', 'ㅓ', 'ㄴ', 'ᴥ', 'ㅅ', 'ᴥ', 'ᴥ', 'ㄴ', 'ㄹ', 'ㄴ', 'ㅌ', 'ᴥ', 'ㄴ', 'ㄹ', 'ᴥ', ' ', 'ㅜ', 'ㄱ', 'ㄴ', 'ㄱ', 'ㅜ', 'ᴥ', 'ㄱ', 'ᴥ', 'ㅁ', ' ', 'ᴥ', 'ㅡ', 'ㅇ', ' ', 'ㄴ', 'ㅏ', ' ', 'ᴥ', 'ㅎ', 'ᴥ', 'ㅔ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㄴ', 'ㄱ', 'ᴥ', 'ㅣ', 'ㄹ', 'ㄹ', 'ㅏ', 'ㅎ', 'ㅓ', 'ㅏ', '\\n', 'ᴥ', 'ㅎ', ' ', ' ', 'ᴥ', 'ᴥ', 'ᴥ', 'ㅇ', 'ㅁ', ' ', 'ㅇ', 'ㄹ', 'ᴥ', 'ㅣ', 'ㅇ', 'ㄴ', 'ㅜ', 'ㄱ', 'ㅇ', 'ᴥ', 'ᴥ', 'ㅏ', 'ᴥ', 'ㅠ', 'ㅇ', 'ㅣ', '\\n', 'ᴥ', 'ㅗ', 'ㄴ', 'ㄵ', 'ᴥ', 'ㅇ', 'ㅏ', ' ', 'ㅆ', 'ㅁ', 'ㄱ', 'ᴥ', 'ㅠ', 'ㄴ', ' ', 'ᴥ', 'ᴥ', 'ㅗ', 'ㄸ', 'ㅣ', 'ㅁ', ' ', 'ᴥ', 'ㅕ', 'ㅂ', 'ㅚ', 'ㅡ', 'ㅢ', 'ㅏ', 'ᴥ', 'ㄹ', 'ㅗ', 'ㅇ', 'ㅔ', 'ᴥ', 'ᴥ', 'ㅇ', ' ', 'ᴥ', 'ㄱ', 'ㄴ', 'ㅣ', 'ㅣ']\n",
      "\n",
      "-- RESULT --\n",
      "누구 ㅡ히ㅏㄴ ㄱㅓㅇ가ㄱㄴ    이 ㄴ ㅓㅅㅓㅢㅓㅇㅇㅈㅇㅇㅎㄸㅅ ㅡㄹㅇㄴㅇㄱㅅ ㅣㅇ올 이ㅊㄱㄷ과ㅔㄴㄴㅇㄷㅇㄱㅅ승ㅓㅜㅣ로ㅔㅘㅗㄱㄱ굥ㅡㅅㅘㅆ 애ㅡㄴㅁㅇ ㄴ어ㅔ ㄱㅜㅗㅅㅇㅅㅅ)ㄱㅣㅏㄱㅅㅕ ㅚㅇ  ㅏㅇㄴㄱㄴㅇ ㅗㅁ\n",
      "ㅅㅇ멓ㅇ뭄ㅣ)ㅔㅜ  ㄹ ㅡㅏㄴㅖㅣㄹㄱㄷ허당ㅡㅇㅄㅐㅡㄱㄹㅎ나ㅡㅠ읃ㄹㅇㄴ\n",
      " ㄴㅇㄹㄹㅂㅇㄱ ㄱ라 ㅇ ㅡㅡㅣㅘㅣㄹ 타ㅏㅇㅏㄱㄴ두아ㅡㅡㅜㅡㄷㅣㅅ ㅕ ㄷㅈㅇ ㅇㅡ ㅇ아ㅣㄴㅇㅢ\n",
      "ㅁㅂ르ㅗ ㅣㅏ.ㅎ ㅏㅔㄴㅂ빘ㅓㅏ ㄹ ㄴㄹ ㅁㅕ  ㅠㄴ둘ㅗㅏㄷㅇ ㅣ숩ㄹㄴ ㅇㅊ장ㅂ솨ㅔ머 ㄴㅇㄴㄴ하ㄴㄷㄹㅇ미 엇 누ㅣㄷㅇㅅ\n",
      "엇ㄹㅏㅍ네써ㅡ개ㅣ ㄸ허ㅡ드   ㅇ히 ㅣㅏㅐㅢㅡㅣ.ㅈㅣ 리ㅐㄱㅜㅉㄹㅏ 성ㅎㅁㄴㅇㄴㅂㄴ ㅁ있ㅗ  ㄹㅎ멩ㄱㅇㄴ ㅇ 아나ㅏㅇ우에ㅈㅎㅁㄴㄷ ㅇㅎㅂㄱㄴㄷ긔ㅕㅐㅣㅏㄴㄴㅇ ㅜ ㅅ\n",
      "ㄴ ㅗㅇㄴㅅㅇㅅㅈㄹ,ㅅ ㅣㅣ ㅏㄱ ㅣ ㅣ 허너ㅓㄴㅇㅏㄹㅅ  ㅂㅣㅣㅔㅆ 가 ㅅ  ㅣㅗㅗ어ㅈㅇㅁㅅㄹㅇ ㅂㄴ ㅅ ㅣㅘㅇ닣ㅜㄴㅈㄱㅈㅇㅇ나ㅣ  ㅇ샤 ㅇ ㄷㅇㄹ  ㄱㅇㄱㄱㅅㅇ루ㅐ리 ㅓㅎㅇㅣ나ㄴㅎㄹ고ㅏㅗ ㅏㄴ닊ㅅㅂㅇ  ㅏ가ㅕㅅㅏㅏㅠㅗㄴㅏㅅㅁㅇㅎ낯ㅗ\"믁ㅇㄴㅓㅕㅇㄷㅇㄴ ㅗㅇㅡㅌㅓㅁ ㅏㅇㅆㄴㅇㅈㄹㅍ유ㅓ라ㅔㅔㅁ으ㅣ바씸 ㅜㅁ으ㅣ ㅜㄹㅇ버ㅢㅐㅂㅗㅜ일ㅗ려 ㅔㅐ넟ㅇ 어 ㄹㅇㄴ,ㅣ ㅈ ㅇㅅㄴㄹㄶ ㅇㅐㅁㄹㅔㅏㄴ두 ㄴㅇㅁ우ㅇ여  ㅢㅓㅗㅏ ㄹ ㄴㅏ ㅈ가 ㅣㅐㄴㅁㄱㅅ ㅡㅏㄹ\n",
      "ㅓㅜㅓㄱ간ㅓㅢㄱㄿㅏㅣㅇㅆㅁ\n",
      "ㅇ ㅇㄴ ㅛㄱㅇㅇㅇ 런  ㅗ ㅅㅊㅏㅏㅡ뎌ㅜㅈㅈㅇ ㅗㅏㅇㅇ이ㅔㅇㄱㅏㅜㅡㅔㅎㅇ\n",
      " ㅓㅏㄴㄴ ㅏㅏㄹ ㅗㄹㄱㅈㅎ네ㅡㅗㄹㅡ개ㅗㅡㅗㄴ  아머ㅢㅣㅣ베ㅏㅡㄹㅇ릳 ㅏㅏㅇㅇㄷㅎㅆㅂ리ㅏㅏ  ㅣㅇ사  ㅗㅎㅇ 므 ㅎ ㄴ내 ㅕㅓㅝㅣ농ㅏㅡㅇㅇㄹㄹㄴ이ㅖㅜㄹㄹㄴㅇㅁ ㄹㅌㄴ개ㅏㄴㄴㄹ샸ㄴㅇ링ㅡㅡ아ㅊ놩ㄴㄴㄹ티ㅡㅡ겨ㅇ즌ㅅㅣㅅㅇㄴ약에ㅕ ㅜㅁ ㅁ  ㅓㅡㄴㅎ ㅁ 버ㅡ므ㅏㅏㅓㅕㅅ ㅕㅁ ㅡㅡㄴㄹ자 ㄱㅇㅎㅁ윙 모 ㅁ   ㅏㅓㅡㄲ ㅡㅏㅏㄹㄹ  ㅎ!ㅜㅇㅏㅁ으ㅣㅇㅁㅓㄷㅗㅈㅁ단ㄴㅂㅣㅇㅡㅈㅓㅁ,ㄱ , \n",
      "ㅏㄹㅇㅕㅣㅅㄱ ㅎㅅㅇㅜㅓㄴ 읒 ㅇㅅㅇㄱ ㅋㄴ뤼ㅣㅔㄴㅏ\n",
      "ㅂ ㅘㅛ\n",
      "ㅕㄱㅡㅐㅂ  ㅇㅏㅈ써ㅏㅆ낫ㅇ.ㄴㄴㅈㅣㄴㅅㅈ로ㅓㅇㅣㅣ몬ㅇㅛㅇㅈㄷㅣㅇㅓ ㅎ으ㅏㅇㅇㅇㄷㅇㅇㅊㅏㅣ ㅈㄴ ㅕㅅㅇ시ㄹ랗ㅕㅣㄴ럮ㅡㅏㅇ   ㅕ\n",
      "ㅂㄱ기ㅣㅓㄱㅣ ㅣ ㅡ앋ㄱㅅ이ㅐㄹ쓱ㅊㅡㅇ\n",
      "ㅇㄴㄱㄴㄹㄴㅇㄹ류 아ㅔㄹㅇㅊ 먼ㅓㅏㅡㅏㅎㄹㄷ  ㅏㅟㅓㅐㅢ아ㅕㅣ ㅓㅉㅎ ㅇ끄ㅗ ㅇㅅㄹㅇㄷㅇㅇㅍ ㅇㅇㄴㅎ\n",
      "ㄱㅁㄴㅏ ㅏㄴㅇㄹ ㅡㄴ송ㅏ ㅇ ㅗ\"ㅔㅓㄹㅏㅅㄹ룹   ㅅ\n",
      "\"ㅗㄴㅌㅏㄹㅌㅇㄱㅆㄹ강ㅔ, ㅕ합ㄱ ㄴㅇ노 ㅜㄹㄱㄴㅕㄴㅇㄹ실ㅇ ㄱㅇ ㅇ ㅣㅏㅜㅣㄷ ㅇㄸㅜㅈㅎ\n",
      "ㄷ\n",
      "룼ㄱ녇ㅓ낮 ㅁㅇㄹ리ㄴㅅㅘㅇ\n",
      "ㅏㅓㄴㅅㄴㄹㄴㅌㄴㄹ ㅜㄱㄴ구ㄱㅁ ㅡㅇ 나 ㅎㅔㄴㄱㅣㄹ랗ㅓㅏ\n",
      "ㅎ  ㅇㅁ ㅇㄹㅣㅇ눅ㅇㅏㅠ이\n",
      "ㅗㄴㄵ아 ㅆㅁㄱㅠㄴ ㅗ띰 ㅕ뵈ㅡㅢㅏ롱ㅔㅇ ㄱ니ㅣ\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'data/nine_dreams'\n",
    "prime = decompose_text(u\"누구 \")\n",
    "\n",
    "print (\"Prime Text : %s => %s\" % (automata(prime), \"\".join(prime)))\n",
    "n = 2000\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "\n",
    "# load_name = u'data/nine_dreams/model.ckpt-0'\n",
    "load_name = u'data/nine_dreams/model.ckpt-99000'\n",
    "\n",
    "print (load_name)\n",
    "\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, load_name)\n",
    "    sampled_text = sample(sess, chars, vocab, probs, n, prime)\n",
    "    #print (\"\")\n",
    "    print (u\"SAMPLED TEXT = %s\" % sampled_text)\n",
    "    print (\"\")\n",
    "    print (\"-- RESULT --\")\n",
    "    print (automata(\"\".join(sampled_text)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
